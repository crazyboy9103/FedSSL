wandb: Currently logged in as: crazyboy9103. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /home/kwangyeongill/FedSSL_clean/wandb/run-20220824_004215-20220824_004213
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FLSL_noniid_bn_fedavg
wandb: ‚≠êÔ∏è View project at https://wandb.ai/crazyboy9103/Fed
wandb: üöÄ View run at https://wandb.ai/crazyboy9103/Fed/runs/20220824_004213
Files already downloaded and verified
Files already downloaded and verified

 | Global Training Round : 1 |

[W CudaIPCTypes.cpp:92] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
Client 92 Linear evaluating FLSL model
Client 69 Linear evaluating FLSL model
Client 98 Linear evaluating FLSL model
Client 91 Linear evaluating FLSL model
Client 94 Linear evaluating FLSL model
Client 85 Linear evaluating FLSL model
Client 92 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.46%
                          test acc/top5 : 87.04%
                          test loss : 1.79
                          train loss : 0.51 
                          time taken : 202.15 
Client 54 Linear evaluating FLSL model
Client 69 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.26%
                          test acc/top5 : 87.02%
                          test loss : 1.79
                          train loss : 0.46 
                          time taken : 203.06 
Client 45 Linear evaluating FLSL model
Client 98 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.30%
                          test acc/top5 : 87.68%
                          test loss : 1.74
                          train loss : 0.49 
                          time taken : 202.71 
Client 79 Linear evaluating FLSL model
Client 91 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.00%
                          test acc/top5 : 87.64%
                          test loss : 1.79
                          train loss : 0.72 
                          time taken : 201.75 
Client 52 Linear evaluating FLSL model
Client 94 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 33.33%
                          test acc/top5 : 86.08%
                          test loss : 1.84
                          train loss : 0.44 
                          time taken : 202.29 
Client 85 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.90%
                          test acc/top5 : 88.40%
                          test loss : 1.76
                          train loss : 0.56 
                          time taken : 204.06 
Client 54 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.74%
                          test acc/top5 : 87.72%
                          test loss : 1.79
                          train loss : 0.91 
                          time taken : 208.17 
Client 45 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.86%
                          test acc/top5 : 88.86%
                          test loss : 1.75
                          train loss : 0.55 
                          time taken : 203.45 
Client 79 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 33.57%
                          test acc/top5 : 86.68%
                          test loss : 1.82
                          train loss : 0.94 
                          time taken : 203.90 
Client 52 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 33.01%
                          test acc/top5 : 86.56%
                          test loss : 1.93
                          train loss : 0.62 
                          time taken : 197.14 
Client 92 Linear evaluating FLSL model
Client 69 Linear evaluating FLSL model
Client 98 Linear evaluating FLSL model
Client 91 Linear evaluating FLSL model
Client 94 Linear evaluating FLSL model
Client 85 Linear evaluating FLSL model
Client 92 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.28%
                          test acc/top5 : 87.64%
                          test loss : 1.75
                          train loss : 0.49 
                          time taken : 204.45 
Training complete best top1/top5: 35.28%/87.64%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 45 Linear evaluating FLSL model
Client 54 Linear evaluating FLSL model
Client 69 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.80%
                          test acc/top5 : 88.24%
                          test loss : 1.73
                          train loss : 0.19 
                          time taken : 207.27 
Training complete best top1/top5: 36.80%/88.24%
Client 98 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.92%
                          test acc/top5 : 88.02%
                          test loss : 1.77
                          train loss : 0.27 
                          time taken : 201.02 
Training complete best top1/top5: 37.92%/88.02%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 52 Linear evaluating FLSL model
Client 79 Linear evaluating FLSL model
Client 91 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.68%
                          test acc/top5 : 88.00%
                          test loss : 1.76
                          train loss : 0.32 
                          time taken : 204.88 
Training complete best top1/top5: 36.68%/88.00%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 94 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.16%
                          test acc/top5 : 88.06%
                          test loss : 1.73
                          train loss : 0.28 
                          time taken : 203.14 
Training complete best top1/top5: 37.16%/88.06%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 85 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.28%
                          test acc/top5 : 88.50%
                          test loss : 1.73
                          train loss : 0.62 
                          time taken : 202.68 
Training complete best top1/top5: 37.28%/88.50%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 45 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.62%
                          test acc/top5 : 88.38%
                          test loss : 1.73
                          train loss : 0.32 
                          time taken : 197.30 
Training complete best top1/top5: 37.62%/88.38%
Client 54 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.36%
                          test acc/top5 : 87.18%
                          test loss : 1.77
                          train loss : 0.44 
                          time taken : 203.40 
Training complete best top1/top5: 36.36%/87.18%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 52 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.20%
                          test acc/top5 : 88.32%
                          test loss : 1.76
                          train loss : 0.25 
                          time taken : 194.95 
Training complete best top1/top5: 36.20%/88.32%
Client 79 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.22%
                          test acc/top5 : 86.88%
                          test loss : 1.78
                          train loss : 0.58 
                          time taken : 199.38 
Training complete best top1/top5: 35.22%/86.88%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
0 summary
1 summary
2 summary
3 summary
4 summary
5 summary
6 summary
7 summary
8 summary
9 summary
l2norm 0.7423433065414429
l2norm 0.09348887205123901
l2norm 1.6640886068344116
l2norm 0.07941456884145737
l2norm 1.8554308414459229
l2norm 0.08275581896305084
l2norm 1.8157185316085815
l2norm 0.08302045613527298
l2norm 1.8859586715698242
l2norm 0.08280234038829803
l2norm 2.5678858757019043
l2norm 0.11871277540922165
l2norm 3.6961066722869873
l2norm 0.12012497335672379
l2norm 0.9263371229171753
l2norm 0.13755811750888824
l2norm 3.692307233810425
l2norm 0.10825038701295853
l2norm 3.5829434394836426
l2norm 0.11429638415575027
l2norm 4.586302280426025
l2norm 0.14088504016399384
l2norm 6.119750499725342
l2norm 0.1471884399652481
l2norm 1.7696605920791626
l2norm 0.16322624683380127
l2norm 5.727823257446289
l2norm 0.11202720552682877
l2norm 5.707815170288086
l2norm 0.1380949318408966
l2norm 5.917844295501709
l2norm 0.21779529750347137
l2norm 4.314450263977051
l2norm 0.1876038759946823
l2norm 3.2368195056915283
l2norm 0.2195373773574829
l2norm 4.157744884490967
l2norm 0.20051567256450653
l2norm 4.065499305725098
l2norm 0.3041395843029022
l2norm 0.6624392867088318
71.54670801013708
l2norm 0.6994777321815491
l2norm 0.08037427812814713
l2norm 1.508495807647705
l2norm 0.06993500888347626
l2norm 1.6447417736053467
l2norm 0.07658608257770538
l2norm 1.6740126609802246
l2norm 0.06706538051366806
l2norm 1.7125365734100342
l2norm 0.08215722441673279
l2norm 2.323500633239746
l2norm 0.10658109188079834
l2norm 3.4084761142730713
l2norm 0.09602341055870056
l2norm 0.8209584355354309
l2norm 0.11746640503406525
l2norm 3.391425848007202
l2norm 0.1045050099492073
l2norm 3.2784647941589355
l2norm 0.11099317669868469
l2norm 4.278784275054932
l2norm 0.12651558220386505
l2norm 5.743622779846191
l2norm 0.12168694287538528
l2norm 1.5887107849121094
l2norm 0.15551969408988953
l2norm 5.436202049255371
l2norm 0.11673460900783539
l2norm 5.4022345542907715
l2norm 0.13054890930652618
l2norm 5.503600597381592
l2norm 0.1904865801334381
l2norm 4.01639461517334
l2norm 0.1729893535375595
l2norm 3.0025546550750732
l2norm 0.20704427361488342
l2norm 3.8786747455596924
l2norm 0.1785910576581955
l2norm 3.7582552433013916
l2norm 0.2605651617050171
l2norm 0.7562662363052368
66.39976014196873
l2norm 0.767424464225769
l2norm 0.08447909355163574
l2norm 1.7196217775344849
l2norm 0.06517291814088821
l2norm 1.850857138633728
l2norm 0.07250531762838364
l2norm 1.83856201171875
l2norm 0.07720731943845749
l2norm 1.903469443321228
l2norm 0.07258474081754684
l2norm 2.6206648349761963
l2norm 0.12122376263141632
l2norm 3.732964038848877
l2norm 0.12111234664916992
l2norm 0.9454050064086914
l2norm 0.13849620521068573
l2norm 3.723797559738159
l2norm 0.11097132414579391
l2norm 3.611264228820801
l2norm 0.11020757257938385
l2norm 4.6024603843688965
l2norm 0.1383044421672821
l2norm 6.083044052124023
l2norm 0.13218404352664948
l2norm 1.794992446899414
l2norm 0.17681221663951874
l2norm 5.740577220916748
l2norm 0.11707363277673721
l2norm 5.7062201499938965
l2norm 0.14357751607894897
l2norm 5.896895408630371
l2norm 0.2082815170288086
l2norm 4.323202610015869
l2norm 0.21800358593463898
l2norm 3.2195541858673096
l2norm 0.216941699385643
l2norm 4.1125006675720215
l2norm 0.1871047466993332
l2norm 4.050253391265869
l2norm 0.2913602590560913
l2norm 0.718498706817627
71.76583398878574
l2norm 0.7529101371765137
l2norm 0.08751728385686874
l2norm 1.6770656108856201
l2norm 0.07199394702911377
l2norm 1.7587891817092896
l2norm 0.08550804108381271
l2norm 1.7840871810913086
l2norm 0.0807868167757988
l2norm 1.8398847579956055
l2norm 0.07288489490747452
l2norm 2.483696699142456
l2norm 0.11814556270837784
l2norm 3.5911848545074463
l2norm 0.11793030053377151
l2norm 0.8877508640289307
l2norm 0.13286620378494263
l2norm 3.561609983444214
l2norm 0.1061960831284523
l2norm 3.4248433113098145
l2norm 0.11683931946754456
l2norm 4.452259063720703
l2norm 0.14569500088691711
l2norm 5.950761795043945
l2norm 0.12608808279037476
l2norm 1.6669280529022217
l2norm 0.1601024568080902
l2norm 5.559982776641846
l2norm 0.12196701765060425
l2norm 5.5537028312683105
l2norm 0.1320248693227768
l2norm 5.566301345825195
l2norm 0.18642300367355347
l2norm 4.0110273361206055
l2norm 0.1965240091085434
l2norm 3.146446466445923
l2norm 0.2056441605091095
l2norm 3.8125665187835693
l2norm 0.18305492401123047
l2norm 3.7659366130828857
l2norm 0.23991082608699799
l2norm 0.6986557841300964
68.63449396938086
l2norm 0.7092007994651794
l2norm 0.09152171015739441
l2norm 1.6095454692840576
l2norm 0.06195176765322685
l2norm 1.706000566482544
l2norm 0.07795270532369614
l2norm 1.7124274969100952
l2norm 0.08128730952739716
l2norm 1.7509020566940308
l2norm 0.08720250427722931
l2norm 2.391055107116699
l2norm 0.09949614107608795
l2norm 3.4342799186706543
l2norm 0.09965596348047256
l2norm 0.8474878072738647
l2norm 0.10288981348276138
l2norm 3.4494175910949707
l2norm 0.10816176980733871
l2norm 3.3242945671081543
l2norm 0.10846006870269775
l2norm 4.3437395095825195
l2norm 0.1279648095369339
l2norm 5.762067794799805
l2norm 0.13074980676174164
l2norm 1.6199806928634644
l2norm 0.15618281066417694
l2norm 5.503474712371826
l2norm 0.10846365988254547
l2norm 5.558689117431641
l2norm 0.14030981063842773
l2norm 5.50274658203125
l2norm 0.18850228190422058
l2norm 4.012642860412598
l2norm 0.18675266206264496
l2norm 3.058727979660034
l2norm 0.2094447910785675
l2norm 3.9108991622924805
l2norm 0.18317662179470062
l2norm 3.8260772228240967
l2norm 0.2829993665218353
l2norm 0.790185272693634
67.4569686613977
l2norm 0.7570071816444397
l2norm 0.08334090560674667
l2norm 1.6587440967559814
l2norm 0.08940168470144272
l2norm 1.794255256652832
l2norm 0.07565566152334213
l2norm 1.8212639093399048
l2norm 0.08023528009653091
l2norm 1.8471777439117432
l2norm 0.07356742024421692
l2norm 2.5294744968414307
l2norm 0.12077075242996216
l2norm 3.7048325538635254
l2norm 0.10860420018434525
l2norm 0.9140199422836304
l2norm 0.11576986312866211
l2norm 3.678152322769165
l2norm 0.11066372692584991
l2norm 3.591886043548584
l2norm 0.11057963967323303
l2norm 4.691117763519287
l2norm 0.14906281232833862
l2norm 6.182099342346191
l2norm 0.13820433616638184
l2norm 1.7216956615447998
l2norm 0.1757134050130844
l2norm 5.658417224884033
l2norm 0.12255297601222992
l2norm 5.655792236328125
l2norm 0.12895433604717255
l2norm 5.900841236114502
l2norm 0.19777841866016388
l2norm 4.355612754821777
l2norm 0.19873446226119995
l2norm 3.1565232276916504
l2norm 0.21023322641849518
l2norm 4.1884636878967285
l2norm 0.19269293546676636
l2norm 4.083521366119385
l2norm 0.31011536717414856
l2norm 0.7115875482559204
71.39511700719595
l2norm 0.740243136882782
l2norm 0.07799879461526871
l2norm 1.6502782106399536
l2norm 0.077940434217453
l2norm 1.746072769165039
l2norm 0.09256761521100998
l2norm 1.7595698833465576
l2norm 0.0772445797920227
l2norm 1.8285222053527832
l2norm 0.09011588990688324
l2norm 2.485747814178467
l2norm 0.11330491304397583
l2norm 3.601876974105835
l2norm 0.11898356676101685
l2norm 0.863311767578125
l2norm 0.1199493408203125
l2norm 3.6400656700134277
l2norm 0.10066153854131699
l2norm 3.474107027053833
l2norm 0.10690739750862122
l2norm 4.510088920593262
l2norm 0.1557542085647583
l2norm 6.038229942321777
l2norm 0.14062203466892242
l2norm 1.6829036474227905
l2norm 0.1647343635559082
l2norm 5.673375129699707
l2norm 0.11689421534538269
l2norm 5.676394939422607
l2norm 0.12664946913719177
l2norm 5.673644065856934
l2norm 0.2036782205104828
l2norm 4.1089677810668945
l2norm 0.18393997848033905
l2norm 3.12422251701355
l2norm 0.2139504998922348
l2norm 3.9244396686553955
l2norm 0.18246148526668549
l2norm 3.80358624458313
l2norm 0.26747170090675354
l2norm 0.7315521836280823
69.46903074532747
l2norm 0.7587206363677979
l2norm 0.07902107387781143
l2norm 1.6128480434417725
l2norm 0.07430042326450348
l2norm 1.7348394393920898
l2norm 0.09204207360744476
l2norm 1.7133820056915283
l2norm 0.07097765803337097
l2norm 1.797849178314209
l2norm 0.0859980508685112
l2norm 2.5005602836608887
l2norm 0.10896718502044678
l2norm 3.6065573692321777
l2norm 0.1132490485906601
l2norm 0.8668884038925171
l2norm 0.12600934505462646
l2norm 3.6303582191467285
l2norm 0.11399421095848083
l2norm 3.5322437286376953
l2norm 0.11748962849378586
l2norm 4.518571853637695
l2norm 0.145187109708786
l2norm 6.072434425354004
l2norm 0.13142620027065277
l2norm 1.7282487154006958
l2norm 0.1571824848651886
l2norm 5.74550199508667
l2norm 0.123977430164814
l2norm 5.702320098876953
l2norm 0.1337803155183792
l2norm 5.701965808868408
l2norm 0.19975516200065613
l2norm 4.169423580169678
l2norm 0.21353831887245178
l2norm 3.21226167678833
l2norm 0.21048609912395477
l2norm 3.994006633758545
l2norm 0.19637534022331238
l2norm 3.9410626888275146
l2norm 0.2947548031806946
l2norm 0.7287402153015137
70.05729696154594
l2norm 0.7539938688278198
l2norm 0.08780930936336517
l2norm 1.6018954515457153
l2norm 0.07754164934158325
l2norm 1.750041127204895
l2norm 0.08881651610136032
l2norm 1.7615678310394287
l2norm 0.06360994279384613
l2norm 1.8028936386108398
l2norm 0.08835906535387039
l2norm 2.4775524139404297
l2norm 0.10821587592363358
l2norm 3.56294322013855
l2norm 0.11256701499223709
l2norm 0.8742755651473999
l2norm 0.12884828448295593
l2norm 3.562741994857788
l2norm 0.1023297980427742
l2norm 3.466156244277954
l2norm 0.10506552457809448
l2norm 4.495580196380615
l2norm 0.14352847635746002
l2norm 6.039192199707031
l2norm 0.12442067265510559
l2norm 1.6933083534240723
l2norm 0.17231155931949615
l2norm 5.620232105255127
l2norm 0.12574033439159393
l2norm 5.629872798919678
l2norm 0.14062464237213135
l2norm 5.783083915710449
l2norm 0.21644149720668793
l2norm 4.236207485198975
l2norm 0.1995571404695511
l2norm 3.1541547775268555
l2norm 0.21550333499908447
l2norm 4.084221363067627
l2norm 0.19193123281002045
l2norm 4.007170677185059
l2norm 0.303044855594635
l2norm 0.666679859161377
69.82003181427717
l2norm 0.7669037580490112
l2norm 0.08371412754058838
l2norm 1.6825259923934937
l2norm 0.0729387104511261
l2norm 1.8402950763702393
l2norm 0.08870314806699753
l2norm 1.8456557989120483
l2norm 0.07693088054656982
l2norm 1.8570759296417236
l2norm 0.08380672335624695
l2norm 2.519007444381714
l2norm 0.1216299831867218
l2norm 3.6472725868225098
l2norm 0.11996091902256012
l2norm 0.9042356014251709
l2norm 0.15297016501426697
l2norm 3.617489814758301
l2norm 0.10575296729803085
l2norm 3.458204984664917
l2norm 0.11960280686616898
l2norm 4.466345310211182
l2norm 0.13126133382320404
l2norm 5.982819080352783
l2norm 0.12385011464357376
l2norm 1.7026958465576172
l2norm 0.1820107400417328
l2norm 5.704668045043945
l2norm 0.11725588142871857
l2norm 5.671232223510742
l2norm 0.13934451341629028
l2norm 5.655344486236572
l2norm 0.19275297224521637
l2norm 4.137993335723877
l2norm 0.19269363582134247
l2norm 3.17034912109375
l2norm 0.2208789885044098
l2norm 3.9134750366210938
l2norm 0.19834312796592712
l2norm 3.864060640335083
l2norm 0.25929245352745056
l2norm 0.7540012001991272
69.94534550607204
(10, 41)
means [0.010693890057846791, 0.001219871217273798, 0.023524260550442037, 0.001062679000404843, 0.02538138564406032, 0.0011968064312857748, 0.025450020930838747, 0.0010888110561896429, 0.026166626204968752, 0.001177955792494342, 0.03574366516027627, 0.0016316716387705208, 0.05166383402042538, 0.0016186041640173002, 0.01270356527279049, 0.001826195293829681, 0.05160813029126169, 0.0015387613046277068, 0.049877753976999575, 0.0016094498443249064, 0.06453053236044806, 0.0020156057694206775, 0.08611386973686062, 0.001889580074661602, 0.024357969381609158, 0.0023881509482200046, 0.0809500907405212, 0.001698632026940575, 0.08080083467231516, 0.0019445655067954072, 0.08198365277037037, 0.0028734891906500915, 0.059845938934312316, 0.0027989426836869916, 0.04520472804666259, 0.0030583954109982053, 0.05739711415442852, 0.002719510925272704, 0.05622887878974133, 0.004037438994712453, 0.010378141028243396]
Client -1 Linear evaluating FLSL model
#######################################################
 
Avg Validation Stats after 1 global rounds
Validation Loss     : 2.37
Validation Accuracy : top1/top5 22.41%/73.07%

#######################################################
model saved at ./checkpoints/FLSL_noniid_bn_fedavg.pth.tar

 | Global Training Round : 2 |

Client 9 Linear evaluating FLSL model
Client 18 Linear evaluating FLSL model
Client 74 Linear evaluating FLSL model
Client 23 Linear evaluating FLSL model
Client 52 Linear evaluating FLSL model
Client 9 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.26%
                          test acc/top5 : 88.64%
                          test loss : 1.74
                          train loss : 0.40 
                          time taken : 213.55 
Client 84 Linear evaluating FLSL model
Client 18 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.60%
                          test acc/top5 : 87.76%
                          test loss : 1.77
                          train loss : 0.50 
                          time taken : 212.73 
Client 85 Linear evaluating FLSL model
Client 54 Linear evaluating FLSL model
Client 74 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 38.32%
                          test acc/top5 : 88.40%
                          test loss : 1.72
                          train loss : 0.54 
                          time taken : 216.99 
Client 48 Linear evaluating FLSL model
Client 23 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.82%
                          test acc/top5 : 87.04%
                          test loss : 1.77
                          train loss : 0.42 
                          time taken : 214.33 
Client 93 Linear evaluating FLSL model
Client 52 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.30%
                          test acc/top5 : 87.44%
                          test loss : 1.77
                          train loss : 0.32 
                          time taken : 220.32 
Client 84 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.88%
                          test acc/top5 : 87.66%
                          test loss : 1.77
                          train loss : 0.60 
                          time taken : 216.87 
Client 85 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.08%
                          test acc/top5 : 88.24%
                          test loss : 1.72
                          train loss : 0.52 
                          time taken : 218.38 
Client 54 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.62%
                          test acc/top5 : 88.16%
                          test loss : 1.71
                          train loss : 0.61 
                          time taken : 218.19 
Client 48 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.82%
                          test acc/top5 : 88.48%
                          test loss : 1.74
                          train loss : 0.40 
                          time taken : 211.45 
Client 93 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.64%
                          test acc/top5 : 88.46%
                          test loss : 1.73
                          train loss : 0.57 
                          time taken : 217.51 
Client 9 Linear evaluating FLSL model
Client 18 Linear evaluating FLSL model
Client 74 Linear evaluating FLSL model
Client 23 Linear evaluating FLSL model
Client 52 Linear evaluating FLSL model
Client 84 Linear evaluating FLSL model
Client 9 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.46%
                          test acc/top5 : 88.48%
                          test loss : 1.74
                          train loss : 0.29 
                          time taken : 214.27 
Training complete best top1/top5: 36.46%/88.48%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 85 Linear evaluating FLSL model
Client 18 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.58%
                          test acc/top5 : 87.82%
                          test loss : 1.76
                          train loss : 0.41 
                          time taken : 213.22 
Training complete best top1/top5: 36.58%/87.82%
Client 48 Linear evaluating FLSL model
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
