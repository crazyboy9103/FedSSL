wandb: Currently logged in as: crazyboy9103. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /home/kwangyeongill/FedSSL_clean/wandb/run-20220823_232539-20220823_232536
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FLSL_noniid_bn_fedavg
wandb: ‚≠êÔ∏è View project at https://wandb.ai/crazyboy9103/Fed
wandb: üöÄ View run at https://wandb.ai/crazyboy9103/Fed/runs/20220823_232536
Files already downloaded and verified
Files already downloaded and verified

 | Global Training Round : 1 |

[W CudaIPCTypes.cpp:92] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
Client 92 Linear evaluating FLSL model
Client 69 Linear evaluating FLSL model
Client 98 Linear evaluating FLSL model
Client 91 Linear evaluating FLSL model
Client 94 Linear evaluating FLSL model
Client 85 Linear evaluating FLSL model
Client 54 Linear evaluating FLSL model
Client 45 Linear evaluating FLSL model
Client 92 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.31%
                          test acc/top5 : 85.06%
                          test loss : 1.84
                          train loss : 0.57 
                          time taken : 320.48 
Client 69 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.72%
                          test acc/top5 : 87.60%
                          test loss : 1.78
                          train loss : 0.42 
                          time taken : 318.41 
Client 79 Linear evaluating FLSL model
Client 98 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 33.99%
                          test acc/top5 : 87.80%
                          test loss : 1.80
                          train loss : 0.36 
                          time taken : 322.62 
Client 52 Linear evaluating FLSL model
Client 91 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 32.79%
                          test acc/top5 : 86.92%
                          test loss : 1.84
                          train loss : 0.64 
                          time taken : 323.70 
Client 94 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.96%
                          test acc/top5 : 87.22%
                          test loss : 1.75
                          train loss : 0.45 
                          time taken : 328.74 
Client 85 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.82%
                          test acc/top5 : 87.92%
                          test loss : 1.74
                          train loss : 0.49 
                          time taken : 316.22 
Client 54 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 38.24%
                          test acc/top5 : 88.24%
                          test loss : 1.73
                          train loss : 0.59 
                          time taken : 313.89 
Client 45 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.32%
                          test acc/top5 : 88.00%
                          test loss : 1.81
                          train loss : 0.56 
                          time taken : 307.75 
Client 79 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.42%
                          test acc/top5 : 88.42%
                          test loss : 1.80
                          train loss : 0.63 
                          time taken : 307.18 
Client 52 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.54%
                          test acc/top5 : 86.36%
                          test loss : 1.85
                          train loss : 0.65 
                          time taken : 316.35 
Client 92 Linear evaluating FLSL model
Client 69 Linear evaluating FLSL model
Client 98 Linear evaluating FLSL model
Client 91 Linear evaluating FLSL model
Client 94 Linear evaluating FLSL model
Client 85 Linear evaluating FLSL model
Client 45 Linear evaluating FLSL model
Client 54 Linear evaluating FLSL model
Client 79 Linear evaluating FLSL model
Client 92 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.82%
                          test acc/top5 : 88.44%
                          test loss : 1.79
                          train loss : 0.26 
                          time taken : 321.58 
Training complete best top1/top5: 35.82%/88.44%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 69 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.68%
                          test acc/top5 : 87.48%
                          test loss : 1.74
                          train loss : 0.21 
                          time taken : 316.21 
Training complete best top1/top5: 37.68%/87.48%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 98 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.40%
                          test acc/top5 : 87.44%
                          test loss : 1.78
                          train loss : 0.16 
                          time taken : 314.17 
Training complete best top1/top5: 35.40%/87.44%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 52 Linear evaluating FLSL model
Client 91 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.20%
                          test acc/top5 : 87.94%
                          test loss : 1.81
                          train loss : 0.46 
                          time taken : 311.68 
Training complete best top1/top5: 35.20%/87.94%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 94 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 37.04%
                          test acc/top5 : 87.04%
                          test loss : 1.75
                          train loss : 0.18 
                          time taken : 314.71 
Training complete best top1/top5: 37.04%/87.04%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 85 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 35.50%
                          test acc/top5 : 87.36%
                          test loss : 1.78
                          train loss : 0.26 
                          time taken : 312.90 
Training complete best top1/top5: 35.50%/87.36%
Client 45 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.90%
                          test acc/top5 : 87.40%
                          test loss : 1.75
                          train loss : 0.14 
                          time taken : 302.14 
Training complete best top1/top5: 34.90%/87.40%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 54 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.92%
                          test acc/top5 : 88.12%
                          test loss : 1.77
                          train loss : 0.28 
                          time taken : 312.66 
Training complete best top1/top5: 36.92%/88.12%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 79 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.38%
                          test acc/top5 : 88.12%
                          test loss : 1.80
                          train loss : 0.29 
                          time taken : 301.87 
Training complete best top1/top5: 36.38%/88.12%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 52 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 34.25%
                          test acc/top5 : 88.26%
                          test loss : 1.86
                          train loss : 0.28 
                          time taken : 305.12 
Training complete best top1/top5: 34.25%/88.26%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
0 summary
1 summary
2 summary
3 summary
4 summary
5 summary
6 summary
7 summary
8 summary
9 summary
l2norm 0.7695672512054443
l2norm 0.06762918084859848
l2norm 1.7202537059783936
l2norm 0.06863359361886978
l2norm 1.8264492750167847
l2norm 0.06675119698047638
l2norm 1.8339251279830933
l2norm 0.07943811267614365
l2norm 1.8748602867126465
l2norm 0.08291308581829071
l2norm 2.5785932540893555
l2norm 0.11669658869504929
l2norm 3.7413814067840576
l2norm 0.12139249593019485
l2norm 0.9249905347824097
l2norm 0.12218815088272095
l2norm 3.728198528289795
l2norm 0.11553428322076797
l2norm 3.5870471000671387
l2norm 0.10985971987247467
l2norm 4.648303031921387
l2norm 0.13946570456027985
l2norm 6.1830339431762695
l2norm 0.13876202702522278
l2norm 1.7582697868347168
l2norm 0.1745099127292633
l2norm 5.740246772766113
l2norm 0.11655297130346298
l2norm 5.740529537200928
l2norm 0.1470937430858612
l2norm 5.832249164581299
l2norm 0.21311435103416443
l2norm 4.256196022033691
l2norm 0.19018322229385376
l2norm 3.3055059909820557
l2norm 0.21895228326320648
l2norm 4.120682239532471
l2norm 0.1898692101240158
l2norm 4.037547588348389
l2norm 0.28918200731277466
l2norm 0.6811537146568298
71.65770610421896
l2norm 0.7669133543968201
l2norm 0.0767078846693039
l2norm 1.66775643825531
l2norm 0.07993096113204956
l2norm 1.7889868021011353
l2norm 0.07883976399898529
l2norm 1.7840521335601807
l2norm 0.07729006558656693
l2norm 1.8263927698135376
l2norm 0.0819871798157692
l2norm 2.4638452529907227
l2norm 0.12991268932819366
l2norm 3.5677831172943115
l2norm 0.11561932414770126
l2norm 0.8795130252838135
l2norm 0.13929979503154755
l2norm 3.5361711978912354
l2norm 0.10805529356002808
l2norm 3.416353464126587
l2norm 0.09548984467983246
l2norm 4.404994010925293
l2norm 0.14702220261096954
l2norm 5.844225883483887
l2norm 0.13426513969898224
l2norm 1.6321932077407837
l2norm 0.15949031710624695
l2norm 5.583584308624268
l2norm 0.11728467047214508
l2norm 5.546619892120361
l2norm 0.12762153148651123
l2norm 5.594987392425537
l2norm 0.17750440537929535
l2norm 3.9896533489227295
l2norm 0.18309929966926575
l2norm 3.046660900115967
l2norm 0.1915513277053833
l2norm 3.843672275543213
l2norm 0.1757284700870514
l2norm 3.706655502319336
l2norm 0.2695099115371704
l2norm 0.7684711813926697
68.3256955370307
l2norm 0.7068578004837036
l2norm 0.09052564203739166
l2norm 1.5229814052581787
l2norm 0.07376694679260254
l2norm 1.61078941822052
l2norm 0.06898698955774307
l2norm 1.623142957687378
l2norm 0.06396087259054184
l2norm 1.653384804725647
l2norm 0.06876280158758163
l2norm 2.308746814727783
l2norm 0.0929424837231636
l2norm 3.3340213298797607
l2norm 0.09946165978908539
l2norm 0.8018882870674133
l2norm 0.11467054486274719
l2norm 3.357086658477783
l2norm 0.09723158180713654
l2norm 3.2920284271240234
l2norm 0.09934578090906143
l2norm 4.3130059242248535
l2norm 0.14098691940307617
l2norm 5.733057022094727
l2norm 0.1281985640525818
l2norm 1.6138135194778442
l2norm 0.15417054295539856
l2norm 5.502985000610352
l2norm 0.11337074637413025
l2norm 5.516689777374268
l2norm 0.13327956199645996
l2norm 5.607757568359375
l2norm 0.19475583732128143
l2norm 4.118368625640869
l2norm 0.2015548199415207
l2norm 3.1020498275756836
l2norm 0.2293575257062912
l2norm 3.987297534942627
l2norm 0.1746283769607544
l2norm 3.9047365188598633
l2norm 0.2551591694355011
l2norm 0.7020013332366943
66.9078079238534
l2norm 0.7422431707382202
l2norm 0.08592011779546738
l2norm 1.5855464935302734
l2norm 0.07514096796512604
l2norm 1.7154121398925781
l2norm 0.08081136643886566
l2norm 1.6930099725723267
l2norm 0.06781011074781418
l2norm 1.7523322105407715
l2norm 0.07860682904720306
l2norm 2.4149417877197266
l2norm 0.12047673761844635
l2norm 3.5151495933532715
l2norm 0.10949254781007767
l2norm 0.8690248131752014
l2norm 0.12884390354156494
l2norm 3.507085084915161
l2norm 0.11109015345573425
l2norm 3.3913018703460693
l2norm 0.11399755626916885
l2norm 4.419245719909668
l2norm 0.14154879748821259
l2norm 5.941081523895264
l2norm 0.12893712520599365
l2norm 1.6623520851135254
l2norm 0.15423889458179474
l2norm 5.578299522399902
l2norm 0.10876461118459702
l2norm 5.560798168182373
l2norm 0.13221892714500427
l2norm 5.545958995819092
l2norm 0.18802960216999054
l2norm 4.095332145690918
l2norm 0.1884593814611435
l2norm 3.130964994430542
l2norm 0.20313426852226257
l2norm 3.956519603729248
l2norm 0.1916176974773407
l2norm 3.914454936981201
l2norm 0.27657365798950195
l2norm 0.6746269464492798
68.35139503329992
l2norm 0.7463179230690002
l2norm 0.07134628295898438
l2norm 1.5855590105056763
l2norm 0.07530752569437027
l2norm 1.6599222421646118
l2norm 0.07394218444824219
l2norm 1.6645917892456055
l2norm 0.08190552145242691
l2norm 1.7029014825820923
l2norm 0.07574620097875595
l2norm 2.3201704025268555
l2norm 0.10505945235490799
l2norm 3.3609840869903564
l2norm 0.11000459641218185
l2norm 0.8359090089797974
l2norm 0.10923054814338684
l2norm 3.3530759811401367
l2norm 0.09918100386857986
l2norm 3.2447633743286133
l2norm 0.10403264313936234
l2norm 4.2599968910217285
l2norm 0.12263969331979752
l2norm 5.726857662200928
l2norm 0.12433057278394699
l2norm 1.5841912031173706
l2norm 0.1621396541595459
l2norm 5.414680004119873
l2norm 0.1065712422132492
l2norm 5.452279567718506
l2norm 0.1272394061088562
l2norm 5.410580635070801
l2norm 0.19141234457492828
l2norm 4.012948989868164
l2norm 0.18706224858760834
l2norm 3.0690364837646484
l2norm 0.2037266045808792
l2norm 3.8667478561401367
l2norm 0.18196050822734833
l2norm 3.839832305908203
l2norm 0.30400002002716064
l2norm 0.7963482737541199
66.52453342825174
l2norm 0.7492121458053589
l2norm 0.07806330919265747
l2norm 1.6273267269134521
l2norm 0.06663427501916885
l2norm 1.7506483793258667
l2norm 0.08337048441171646
l2norm 1.7522177696228027
l2norm 0.08920107781887054
l2norm 1.7801222801208496
l2norm 0.08343322575092316
l2norm 2.489664077758789
l2norm 0.10312514007091522
l2norm 3.583547592163086
l2norm 0.11426939815282822
l2norm 0.8595659732818604
l2norm 0.1112200915813446
l2norm 3.564486026763916
l2norm 0.1072133481502533
l2norm 3.452768087387085
l2norm 0.10625126212835312
l2norm 4.497354984283447
l2norm 0.1440727561712265
l2norm 6.049181938171387
l2norm 0.13176016509532928
l2norm 1.7246499061584473
l2norm 0.1777312308549881
l2norm 5.71473503112793
l2norm 0.11028670519590378
l2norm 5.741410732269287
l2norm 0.14262287318706512
l2norm 5.880100250244141
l2norm 0.21373692154884338
l2norm 4.295890808105469
l2norm 0.19060462713241577
l2norm 3.1448142528533936
l2norm 0.21156735718250275
l2norm 4.1645588874816895
l2norm 0.17975938320159912
l2norm 4.069960594177246
l2norm 0.3031952679157257
l2norm 0.7163591980934143
70.35669454187155
l2norm 0.7456719279289246
l2norm 0.08643990010023117
l2norm 1.6721957921981812
l2norm 0.0803893506526947
l2norm 1.7537295818328857
l2norm 0.07939288020133972
l2norm 1.7633041143417358
l2norm 0.08154771476984024
l2norm 1.7737789154052734
l2norm 0.07719502598047256
l2norm 2.4549648761749268
l2norm 0.10318832099437714
l2norm 3.514660358428955
l2norm 0.11170513182878494
l2norm 0.8646026849746704
l2norm 0.11855233460664749
l2norm 3.50073504447937
l2norm 0.10211821645498276
l2norm 3.388672351837158
l2norm 0.10937081277370453
l2norm 4.451125144958496
l2norm 0.13992007076740265
l2norm 5.887454986572266
l2norm 0.12995274364948273
l2norm 1.6689659357070923
l2norm 0.1584038883447647
l2norm 5.534904479980469
l2norm 0.12773683667182922
l2norm 5.5550689697265625
l2norm 0.12863828241825104
l2norm 5.6217041015625
l2norm 0.20040784776210785
l2norm 4.102472305297852
l2norm 0.18345481157302856
l2norm 3.107832431793213
l2norm 0.18221734464168549
l2norm 3.9411072731018066
l2norm 0.17526696622371674
l2norm 3.8420629501342773
l2norm 0.3063184320926666
l2norm 0.7123209834098816
68.53955212235451
l2norm 0.7320526838302612
l2norm 0.08647006750106812
l2norm 1.6067909002304077
l2norm 0.07508586347103119
l2norm 1.7456634044647217
l2norm 0.08056794106960297
l2norm 1.7068734169006348
l2norm 0.09194149076938629
l2norm 1.7763726711273193
l2norm 0.08553312718868256
l2norm 2.4767770767211914
l2norm 0.09741716086864471
l2norm 3.592865467071533
l2norm 0.10399731248617172
l2norm 0.8688891530036926
l2norm 0.10515401512384415
l2norm 3.6126766204833984
l2norm 0.10663682222366333
l2norm 3.481128215789795
l2norm 0.12801747024059296
l2norm 4.50546932220459
l2norm 0.14950992166996002
l2norm 6.01940393447876
l2norm 0.14921002089977264
l2norm 1.744821310043335
l2norm 0.1698525846004486
l2norm 5.69944953918457
l2norm 0.12011024355888367
l2norm 5.70509672164917
l2norm 0.14779207110404968
l2norm 5.824782848358154
l2norm 0.19869060814380646
l2norm 4.177496433258057
l2norm 0.20351214706897736
l2norm 3.1445906162261963
l2norm 0.2092273235321045
l2norm 4.02587366104126
l2norm 0.1739015281200409
l2norm 3.915311813354492
l2norm 0.3033640682697296
l2norm 0.7247471809387207
69.87312477827072
l2norm 0.7648677825927734
l2norm 0.08006415516138077
l2norm 1.6897114515304565
l2norm 0.08079283684492111
l2norm 1.8255027532577515
l2norm 0.08949093520641327
l2norm 1.807267665863037
l2norm 0.07106209546327591
l2norm 1.8534519672393799
l2norm 0.08423925191164017
l2norm 2.535078525543213
l2norm 0.11734145879745483
l2norm 3.625643491744995
l2norm 0.12004151940345764
l2norm 0.8901013731956482
l2norm 0.11697772145271301
l2norm 3.629535675048828
l2norm 0.1150868609547615
l2norm 3.536726951599121
l2norm 0.11126653105020523
l2norm 4.539053916931152
l2norm 0.14169371128082275
l2norm 6.054771900177002
l2norm 0.13239815831184387
l2norm 1.73766028881073
l2norm 0.17450928688049316
l2norm 5.711865425109863
l2norm 0.11496137082576752
l2norm 5.723948955535889
l2norm 0.13366660475730896
l2norm 5.842763423919678
l2norm 0.19854767620563507
l2norm 4.234744071960449
l2norm 0.19534768164157867
l2norm 3.2836718559265137
l2norm 0.2118721604347229
l2norm 4.0083231925964355
l2norm 0.19610550999641418
l2norm 3.933917999267578
l2norm 0.2875216603279114
l2norm 0.745066225528717
70.74666208028793
l2norm 0.7548606395721436
l2norm 0.09580705314874649
l2norm 1.7677884101867676
l2norm 0.07940968871116638
l2norm 1.8632386922836304
l2norm 0.07957620918750763
l2norm 1.8443442583084106
l2norm 0.0962337777018547
l2norm 1.9144355058670044
l2norm 0.06988481432199478
l2norm 2.643648386001587
l2norm 0.12299036234617233
l2norm 3.7996633052825928
l2norm 0.12060440331697464
l2norm 0.9474977850914001
l2norm 0.14312566816806793
l2norm 3.8327369689941406
l2norm 0.11983434855937958
l2norm 3.713543653488159
l2norm 0.1312960684299469
l2norm 4.76689338684082
l2norm 0.14381501078605652
l2norm 6.220554351806641
l2norm 0.13570882380008698
l2norm 1.8331401348114014
l2norm 0.1789231151342392
l2norm 5.786584854125977
l2norm 0.11713838577270508
l2norm 5.7841620445251465
l2norm 0.14501740038394928
l2norm 5.977051734924316
l2norm 0.21501897275447845
l2norm 4.379977703094482
l2norm 0.19747091829776764
l2norm 3.314011335372925
l2norm 0.21502281725406647
l2norm 4.127862453460693
l2norm 0.20526444911956787
l2norm 4.0213398933410645
l2norm 0.2767757773399353
l2norm 0.6659794449806213
72.84823300689459
grads [[0.010739490461586725, 0.0009437809905642054, 0.02400654164782301, 0.0009577978050127517, 0.025488525579654992, 0.0009315285209296742, 0.025592852851245792, 0.0011085773881822126, 0.026164112537817634, 0.0011570714487804275, 0.03598487021534083, 0.0016285281100866637, 0.052211850060377214, 0.001694060590687087, 0.012908458630214921, 0.0017051641411045242, 0.052027879916606644, 0.0016123078661314518, 0.0500580788177916, 0.0015331180112393593, 0.06486815284263908, 0.0019462764319784524, 0.08628568062426763, 0.0019364564478718772, 0.02453706492191488, 0.002435326529647155, 0.08010648239866204, 0.0016265238958940208, 0.08011042844229345, 0.0020527274885401453, 0.08139039723234898, 0.0029740604691449505, 0.05939620807626022, 0.0026540512197983468, 0.04612910698222084, 0.0030555301748671987, 0.057505081638244894, 0.0026496691067373836, 0.05634491819311352, 0.004035602352274414, 0.009505658940102826], 
[0.01122437683757165, 0.0011226798946778415, 0.02440892002850422, 0.0011698521398692402, 0.026183221232362754, 0.0011538816162691861, 0.02611099849826296, 0.00113120056779631, 0.026730686829579096, 0.0011999465087235643, 0.03606030254979819, 0.0019013738287930705, 0.05221729671761144, 0.0016921792488016267, 0.012872361098865668, 0.0020387614635558418, 0.051754631549630184, 0.0015814737444050607, 0.05000100529199904, 0.0013975685710814246, 0.06447053303010877, 0.0021517849391125107, 0.08553481728285478, 0.001965075344549023, 0.023888424331607115, 0.002334265547575838, 0.08172012395538827, 0.0017165528949292573, 0.08117912080549904, 0.0018678409415875433, 0.08188701700655508, 0.002597915820455757, 0.05839169755338157, 0.0026798014748350544, 0.044590265436299265, 0.0028035035165001375, 0.05625515035496485, 0.002571923618279323, 0.054249802701392597, 0.003944488371744467, 0.011247176854221394], [0.010564653400215503, 0.0013529907023768781, 0.02276238682025657, 0.0011025162694995986, 0.024074760004897056, 0.0010310753213773787, 0.024259395249275666, 0.0009559552849696494, 0.02471138804319124, 0.001027724621703933, 0.03450638851231425, 0.0013891126702124184, 0.04983007863109418, 0.0014865478764792437, 0.011984973233318723, 0.0017138589414444983, 0.050174811619869905, 0.0014532172675242043, 0.0492024552780241, 0.0014848159578344745, 0.06446192242814784, 0.0021071818637898123, 0.08568591917731662, 0.0019160478878411668, 0.024119958037102294, 0.002304223494077961, 0.08224727683311941, 0.001694432232829321, 0.08245210758739421, 0.0019919881719655664, 0.0838132012147441, 0.002910808818350911, 0.061552885282505625, 0.003012425996244066, 0.046363046762884115, 0.003427963534051496, 0.05959390478732318, 0.0026099850283467048, 0.0583599528967348, 0.003813593321214368, 0.010492068938136933], [0.010859224897701194, 0.001257035320985156, 0.023196988046225178, 0.0010993333483320762, 0.02509695872420528, 0.0011822928617549854, 0.024769208759345934, 0.0009920808597217066, 0.025637109669627924, 0.001150039863983856, 0.03533127285175669, 0.0017626083207190084, 0.05142762033811798, 0.0016019065559193678, 0.012714075736886184, 0.0018850222951381438, 0.05130963432723728, 0.0016252799727293431, 0.04961569355963943, 0.0016678160879325245, 0.06465479918524952, 0.0020708984420764465, 0.08691968204893032, 0.001886386154125708, 0.024320675303022694, 0.002256558106921615, 0.08161208004141285, 0.0015912566397746277, 0.0813560303410519, 0.0019343998331063895, 0.08113892910477061, 0.0027509255967399777, 0.059915853124807836, 0.0027572133878076446, 0.04580689235245566, 0.002971911084233148, 0.05788498686532561, 0.002803420433247734, 0.05726956904206752, 0.004046349863887326, 0.009869980651025633], [0.011218687070897258, 0.0010724807718634059, 0.023834199637277256, 0.0011320263640118753, 0.024952031327733798, 0.00111150248844647, 0.025022224185020498, 0.0012312077549670292, 0.025598097345826725, 0.0011386205520772272, 0.03487691356797283, 0.0015792587627573075, 0.05052247514994233, 0.001653594407104326, 0.012565424602058197, 0.0016419588761369447, 0.050403600120796144, 0.0014908936411489287, 0.04877543978310146, 0.001563823717028605, 0.06403647904748155, 0.0018435257941653552, 0.08608640101741528, 0.0018689431759493721, 0.023813638690543506, 0.002437291113577178, 0.08139373138121639, 0.0016019840609357865, 0.08195892983749997, 0.0019126688990023036, 0.08133210946764832, 0.002877319609935648, 0.06032284306354772, 0.002811928756920322, 0.04613390467555365, 0.0030624281611926385, 0.058125140559016686, 0.00273523914938234, 0.05772054470775886, 0.004569742985947158, 0.011970745719141345], [0.010648768403403012, 0.0011095363376714558, 0.023129664312824948, 0.000947092177269821, 0.024882470541364037, 0.0011849687503738538, 0.024904776738480815, 0.0012678406568089138, 0.025301391597660136, 0.0011858605111311666, 0.035386313896215084, 0.0014657473711978057, 0.05093399591179487, 0.0016241439268415716, 0.012217259194436782, 0.0015808032527047434, 0.05066306838281857, 0.0015238542522268035, 0.0490751890757493, 0.0015101798460005757, 0.06392220404281394, 0.002047747653714518, 0.08597876829718488, 0.0018727452441205086, 0.024512946740726318, 0.0025261452660942517, 0.0812251779072268, 0.0015675367626924058, 0.08160432734446312, 0.0020271400485164298, 0.08357556148043173, 0.0030379045368830056, 0.061058735577022384, 0.0027091185618303994, 0.044698152369591675, 0.0030070678925456363, 0.05919207709514019, 0.002554971980592671, 0.05784752425734101, 0.004309401825796185, 0.010181819978297669], [0.010879439751776843, 0.0012611681492450018, 0.024397530191224964, 0.0011728899323588567, 0.02558711762081822, 0.0011583513131164065, 0.025726811158525584, 0.0011897905989269378, 0.025879639718666133, 0.001126284365597642, 0.0358182217443208, 0.0015055295489846332, 0.05127930150688323, 0.0016297908050139092, 0.012614653265186366, 0.0017296922862147077, 0.05107612956428392, 0.0014899165998733803, 0.04944112190560881, 0.0015957328197660151, 0.06494243115292754, 0.002041450030452811, 0.08589864981991972, 0.0018960255739269415, 0.024350406211113115, 0.0023111310687001195, 0.08075489711546033, 0.0018636952345967728, 0.08104909935521376, 0.0018768474323936388, 0.08202131364276823, 0.0029239736992203638, 0.059855545860210115, 0.0026766269386402056, 0.045343634960514696, 0.0026585721528555833, 0.05750121135992068, 0.0025571653271214264, 0.05605614322188093, 0.004469221385424831, 0.010392845610345834], [0.010476884870303044, 0.001237529705097127, 0.022995835742701615, 0.0010746029136281241, 0.024983331001787276, 0.0011530605125399823, 0.024428182113181256, 0.001315834822918617, 0.025422831407129785, 0.0012241205393362029, 0.035446777063152385, 0.0013942007199159883, 0.05141984816727203, 0.0014883735744778514, 0.012435241099649538, 0.0015049279026454123, 0.05170337854429083, 0.0015261493251096945, 0.049820703265189635, 0.0018321417661916856, 0.06448071896744068, 0.0021397342990513413, 0.08614762762621839, 0.002135442222932819, 0.024971279237621026, 0.0024308714565069757, 0.08156855095962441, 0.0017189762722081062, 0.0816493714822863, 0.0021151490157773844, 0.08336227794079644, 0.0028435912773947625, 0.05978688439245503, 0.0029125954752243447, 0.045004293513492714, 0.0029943891044811326, 0.057616911706992006, 0.002488818536052092, 0.056034588774711326, 0.004341641643083785, 0.010372331039130855], [0.010811362120869413, 0.0011317022288700877, 0.023883974195317677, 0.0011420021025618443, 0.025803376436135627, 0.0012649492226905789, 0.025545624524476247, 0.0010044586327285661, 0.026198436968460245, 0.0011907169813331964, 0.03583318917104867, 0.0016586147720197468, 0.051248262252010957, 0.0016967799734102888, 0.012581531722097404, 0.0016534733655696582, 0.05130327803917864, 0.001626746161170875, 0.049991432070468744, 0.0015727460176698188, 0.06415926608353532, 0.002002832460420811, 0.08558385261068079, 0.00187144035377372, 0.02456172825282866, 0.0024666787343613278, 0.08073688930550059, 0.0016249723654142418, 0.0809076893131718, 0.0018893697713344463, 0.08258712499098443, 0.002806459985070535, 0.059857863925150065, 0.002761228245933149, 0.04641451284584972, 0.0029948008033831553, 0.05665741781637031, 0.0027719401061474937, 0.0556057046875669, 0.004064102133915703, 0.01053146825051855], [0.010362099510371118, 0.0013151596022882121, 0.024266730121229686, 0.0010900702108128113, 0.02557699226702297, 0.0010923560655201655, 0.02531762518019972, 0.0013210173223109863, 0.026279779575241257, 0.0009593206511320688, 0.036289807959396674, 0.0016883094794424476, 0.052158620030261824, 0.0016555570168127517, 0.013006462147156348, 0.0019647102237129355, 0.05261262779882936, 0.0016449863450776914, 0.05097644102275888, 0.0018023233098532483, 0.06543595074419535, 0.001974172946273733, 0.08539060036250855, 0.0018628979482212431, 0.02516382428435714, 0.002456107825117808, 0.0794334277617731, 0.0016079784085033157, 0.07940016944512182, 0.001990678351391507, 0.08204799880813347, 0.002951601759978562, 0.06012469379566073, 0.0027107166522361407, 0.045491993403042136, 0.002951654534073819, 0.05666386517666145, 0.002817699766309239, 0.055201612000121844, 0.0037993478484747924, 0.00914201233841308]]
Client -1 Linear evaluating FLSL model
#######################################################
 
Avg Validation Stats after 1 global rounds
Validation Loss     : 2.46
Validation Accuracy : top1/top5 22.04%/72.12%

#######################################################
model saved at ./checkpoints/FLSL_noniid_bn_fedavg.pth.tar

 | Global Training Round : 2 |

Client 9 Linear evaluating FLSL model
Client 18 Linear evaluating FLSL model
Client 74 Linear evaluating FLSL model
Client 23 Linear evaluating FLSL model
Client 9 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 38.72%
                          test acc/top5 : 88.76%
                          test loss : 1.69
                          train loss : 0.32 
                          time taken : 300.47 
Client 52 Linear evaluating FLSL model
Client 84 Linear evaluating FLSL model
Client 18 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 36.66%
                          test acc/top5 : 88.24%
                          test loss : 1.76
                          train loss : 0.41 
                          time taken : 317.52 
slurmstepd: error: *** JOB 170429 ON b01 CANCELLED AT 2022-08-23T23:44:35 ***
