wandb: Currently logged in as: crazyboy9103. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /home/kwangyeongill/FedSSL_clean/wandb/run-20220830_020140-20220830_020138
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run simclr_iid_bn_fedprox
wandb: ‚≠êÔ∏è View project at https://wandb.ai/crazyboy9103/Fed
wandb: üöÄ View run at https://wandb.ai/crazyboy9103/Fed/runs/20220830_020138
Files already downloaded and verified
Files already downloaded and verified

 | Global Training Round : 1 |

[W CudaIPCTypes.cpp:92] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
Client 9 Linear evaluating simclr model
Client 7 Linear evaluating simclr model
Client 58 Linear evaluating simclr model
Client 49 Linear evaluating simclr model
Client 9 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.76%
                          test acc/top5 : 83.53%
                          test loss : 1.96
                          train loss : 2.41 
                          time taken : 416.53 
Client 67 Linear evaluating simclr model
Client 42 Linear evaluating simclr model
Client 53 Linear evaluating simclr model
Client 7 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 27.58%
                          test acc/top5 : 82.65%
                          test loss : 1.94
                          train loss : 3.09 
                          time taken : 436.58 
Client 58 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 24.48%
                          test acc/top5 : 83.27%
                          test loss : 1.97
                          train loss : 4.03 
                          time taken : 445.22 
Client 66 Linear evaluating simclr model
Client 49 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.34%
                          test acc/top5 : 81.55%
                          test loss : 1.95
                          train loss : 2.17 
                          time taken : 457.33 
Client 26 Linear evaluating simclr model
Client 67 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 24.18%
                          test acc/top5 : 81.11%
                          test loss : 1.98
                          train loss : 2.90 
                          time taken : 525.58 
Client 56 Linear evaluating simclr model
Client 42 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.62%
                          test acc/top5 : 82.39%
                          test loss : 1.96
                          train loss : 3.88 
                          time taken : 535.73 
Client 53 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.06%
                          test acc/top5 : 82.71%
                          test loss : 1.94
                          train loss : 2.62 
                          time taken : 528.45 
Client 26 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 25.42%
                          test acc/top5 : 83.35%
                          test loss : 1.95
                          train loss : 3.29 
                          time taken : 519.35 
Client 66 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 25.82%
                          test acc/top5 : 81.95%
                          test loss : 1.96
                          train loss : 2.17 
                          time taken : 543.65 
Client 56 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 24.48%
                          test acc/top5 : 82.39%
                          test loss : 1.98
                          train loss : 2.96 
                          time taken : 541.93 
Client 9 Linear evaluating simclr model
Client 7 Linear evaluating simclr model
Client 58 Linear evaluating simclr model
Client 49 Linear evaluating simclr model
Client 67 Linear evaluating simclr model
Client 9 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 28.39%
                          test acc/top5 : 82.97%
                          test loss : 1.95
                          train loss : 1.64 
                          time taken : 505.69 
Training complete best top1/top5: 28.39%/82.97%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 42 Linear evaluating simclr model
Client 53 Linear evaluating simclr model
Client 7 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 28.17%
                          test acc/top5 : 83.53%
                          test loss : 1.93
                          train loss : 2.03 
                          time taken : 498.67 
Training complete best top1/top5: 28.17%/83.53%
Client 58 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.60%
                          test acc/top5 : 83.47%
                          test loss : 1.95
                          train loss : 2.24 
                          time taken : 486.04 
Training complete best top1/top5: 26.60%/83.47%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 49 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.22%
                          test acc/top5 : 82.29%
                          test loss : 1.98
                          train loss : 1.46 
                          time taken : 479.27 
Training complete best top1/top5: 26.22%/82.29%
Client 26 Linear evaluating simclr model
Client 66 Linear evaluating simclr model
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 67 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 28.89%
                          test acc/top5 : 82.39%
                          test loss : 1.92
                          train loss : 1.61 
                          time taken : 429.21 
Training complete best top1/top5: 28.89%/82.39%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 53 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 27.56%
                          test acc/top5 : 83.57%
                          test loss : 1.96
                          train loss : 1.67 
                          time taken : 415.45 
Training complete best top1/top5: 27.56%/83.57%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 56 Linear evaluating simclr model
Client 42 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 23.26%
                          test acc/top5 : 82.07%
                          test loss : 1.96
                          train loss : 2.43 
                          time taken : 429.30 
Training complete best top1/top5: 23.26%/82.07%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 26 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 26.18%
                          test acc/top5 : 81.23%
                          test loss : 1.94
                          train loss : 1.82 
                          time taken : 409.46 
Training complete best top1/top5: 26.18%/81.23%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 66 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 25.60%
                          test acc/top5 : 83.07%
                          test loss : 1.98
                          train loss : 1.68 
                          time taken : 422.38 
Training complete best top1/top5: 25.60%/83.07%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 56 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 28.25%
                          test acc/top5 : 80.97%
                          test loss : 1.93
                          train loss : 1.95 
                          time taken : 418.88 
Training complete best top1/top5: 28.25%/80.97%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
0 summary
1 summary
2 summary
3 summary
4 summary
5 summary
6 summary
7 summary
8 summary
9 summary
Client -1 Linear evaluating simclr model
#######################################################
 
Avg Validation Stats after 1 global rounds
Validation Loss     : 1.93
Validation Accuracy : top1/top5 28.02%/82.99%

#######################################################
model saved at ./checkpoints/simclr_iid_bn_fedprox.pth.tar

 | Global Training Round : 2 |

slurmstepd: error: *** JOB 171632 ON b01 CANCELLED AT 2022-08-30T02:20:48 ***
