wandb: Currently logged in as: crazyboy9103. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /home/kwangyeongill/FedSSL_clean/wandb/run-20220830_014245-20220830_014243
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run simsiam_iid_bn_fedavg
wandb: ‚≠êÔ∏è View project at https://wandb.ai/crazyboy9103/Fed
wandb: üöÄ View run at https://wandb.ai/crazyboy9103/Fed/runs/20220830_014243
Files already downloaded and verified
Files already downloaded and verified

 | Global Training Round : 1 |

[W CudaIPCTypes.cpp:92] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
Client 9 Linear evaluating simsiam model
Client 7 Linear evaluating simsiam model
Client 58 Linear evaluating simsiam model
Client 49 Linear evaluating simsiam model
Client 67 Linear evaluating simsiam model
Client 9 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 20.73%
                          test acc/top5 : 81.03%
                          test loss : 2.03
                          train loss : -0.77 
                          time taken : 205.62 
Client 42 Linear evaluating simsiam model
Client 7 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.55%
                          test acc/top5 : 81.11%
                          test loss : 2.07
                          train loss : -0.83 
                          time taken : 207.69 
Client 53 Linear evaluating simsiam model
Client 58 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 23.42%
                          test acc/top5 : 79.77%
                          test loss : 2.10
                          train loss : -0.80 
                          time taken : 207.25 
Client 66 Linear evaluating simsiam model
Client 49 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.31%
                          test acc/top5 : 77.40%
                          test loss : 2.23
                          train loss : -0.83 
                          time taken : 207.25 
Client 26 Linear evaluating simsiam model
Client 67 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 19.21%
                          test acc/top5 : 79.83%
                          test loss : 2.07
                          train loss : -0.83 
                          time taken : 206.51 
Client 56 Linear evaluating simsiam model
Client 42 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 22.92%
                          test acc/top5 : 82.49%
                          test loss : 1.97
                          train loss : -0.84 
                          time taken : 205.21 
Client 53 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 20.69%
                          test acc/top5 : 80.27%
                          test loss : 2.08
                          train loss : -0.73 
                          time taken : 204.39 
Client 66 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 23.46%
                          test acc/top5 : 76.28%
                          test loss : 2.14
                          train loss : -0.85 
                          time taken : 204.64 
Client 26 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.49%
                          test acc/top5 : 81.31%
                          test loss : 2.05
                          train loss : -0.83 
                          time taken : 207.94 
Client 56 Epoch [5/10]:
                          learning rate : 0.001000
                          test acc/top1 : 23.06%
                          test acc/top5 : 80.89%
                          test loss : 2.03
                          train loss : -0.81 
                          time taken : 206.70 
Client 9 Linear evaluating simsiam model
Client 7 Linear evaluating simsiam model
Client 58 Linear evaluating simsiam model
Client 49 Linear evaluating simsiam model
Client 67 Linear evaluating simsiam model
Client 9 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 24.40%
                          test acc/top5 : 79.55%
                          test loss : 2.09
                          train loss : -0.86 
                          time taken : 200.48 
Training complete best top1/top5: 24.40%/79.55%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 42 Linear evaluating simsiam model
Client 7 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 20.87%
                          test acc/top5 : 81.71%
                          test loss : 2.13
                          train loss : -0.90 
                          time taken : 198.39 
Training complete best top1/top5: 20.87%/81.71%
Client 53 Linear evaluating simsiam model
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 58 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.21%
                          test acc/top5 : 74.92%
                          test loss : 2.26
                          train loss : -0.81 
                          time taken : 197.00 
Training complete best top1/top5: 21.21%/74.92%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 66 Linear evaluating simsiam model
Client 49 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 19.99%
                          test acc/top5 : 79.41%
                          test loss : 2.08
                          train loss : -0.86 
                          time taken : 197.26 
Training complete best top1/top5: 19.99%/79.41%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 26 Linear evaluating simsiam model
Client 67 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 22.88%
                          test acc/top5 : 80.43%
                          test loss : 2.08
                          train loss : -0.83 
                          time taken : 196.84 
Training complete best top1/top5: 22.88%/80.43%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 56 Linear evaluating simsiam model
Client 42 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 18.69%
                          test acc/top5 : 80.17%
                          test loss : 2.05
                          train loss : -0.90 
                          time taken : 195.48 
Training complete best top1/top5: 18.69%/80.17%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 53 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 22.72%
                          test acc/top5 : 80.05%
                          test loss : 2.09
                          train loss : -0.85 
                          time taken : 194.22 
Training complete best top1/top5: 22.72%/80.05%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 66 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 23.22%
                          test acc/top5 : 77.70%
                          test loss : 2.08
                          train loss : -0.87 
                          time taken : 195.20 
Training complete best top1/top5: 23.22%/77.70%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 26 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.33%
                          test acc/top5 : 81.65%
                          test loss : 2.07
                          train loss : -0.90 
                          time taken : 192.41 
Training complete best top1/top5: 21.33%/81.65%
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
Client 56 Epoch [10/10]:
                          learning rate : 0.001000
                          test acc/top1 : 21.03%
                          test acc/top5 : 80.89%
                          test loss : 2.21
                          train loss : -0.93 
                          time taken : 190.24 
Training complete best top1/top5: 21.03%/80.89%
0 summary
1 summary
2 summary
3 summary
4 summary
5 summary
6 summary
7 summary
8 summary
9 summary
Client -1 Linear evaluating simsiam model
#######################################################
 
Avg Validation Stats after 1 global rounds
Validation Loss     : 2.09
Validation Accuracy : top1/top5 26.74%/81.23%

#######################################################
model saved at ./checkpoints/simsiam_iid_bn_fedavg.pth.tar

 | Global Training Round : 2 |

Client 10 Linear evaluating simsiam model
slurmstepd: error: *** JOB 171625 ON b03 CANCELLED AT 2022-08-30T01:54:28 ***
